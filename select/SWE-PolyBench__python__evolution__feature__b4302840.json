{
  "instance_id": "SWE-PolyBench/python/evolution/feature/b4302840",
  "version": "v0.1",
  "category": "evolution",
  "subcategory": "feature",
  "languages": [
    "python"
  ],
  "repo": {
    "origin": "github://huggingface/transformers",
    "commit_pre": "99ba36e72fe7d1528e2c6572373a425967ee544f"
  },
  "instruction": {
    "type": "issue",
    "text": "Make schedulers picklable\n### Feature request\n\nChange lambda functions passed to `LambdaLR` in `get_constant_schedule`, `get_constant_schedule_with_warmup`, `get_linear_schedule_with_warmup`, `get_cosine_schedule_with_warmup`, `get_cosine_with_hard_restarts_schedule_with_warmup` and `get_polynomial_decay_schedule_with_warmup` to callable objects.\n\n### Motivation\n\nPython cannot serialize lambda and local functions. Torch created a workaround around this in their `state_dict` method of `LambdaLR` by not returning any non-picklable functions:\r\n```python\r\n        ...\r\n        for idx, fn in enumerate(self.lr_lambdas):\r\n            if not isinstance(fn, types.FunctionType):\r\n                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()\r\n\r\n        return state_dict\r\n```\r\nWhile this approach is fine when LR schedule is constant and deterministic, it makes it impossible to change the schedule mid training dynamically using lambda functions since any changes will not be saved to checkpoints.\r\n\r\nIn my particular case I wanted to implement a dynamic LR schedule based on evaluation metrics. I've implemented a wrapper around `LambdaLR` that applies transformation `fn: float -> float` to existing LR schedule:\r\n```python\r\nclass LambdaWrapper:\r\n\r\n    def __init__(self, lr_lamda: Callable[[Union[float, int]], float], wrapper_function: Callable[[float], float]):\r\n        self._wrapper_function = wrapper_function\r\n        self._lr_lambda = lr_lamda\r\n\r\n    def __call__(self, x: Union[float, int]):\r\n        return self._wrapper_function(self._lr_lambda(x))\r\n\r\n\r\nclass DynamicScheduler:\r\n\r\n    def __init__(self, lr_scheduler: LambdaLR):\r\n        self._scheduler = lr_scheduler\r\n\r\n    def __getattr__(self, item):\r\n        # Calling the super class to avoid recursion\r\n        return getattr(super(DynamicScheduler, self).__getattribute__('_scheduler'), item)\r\n\r\n    def wrap_schedule(self, fn: Callable[[float], float]):\r\n        \"\"\"If you want this object to be picklable, pass only picklable callable objects as `fn`!\"\"\"\r\n        wrappers_builder = partial(LambdaWrapper, wrapper_function=fn)  # wrap in callable object to preserve picklability\r\n        self._scheduler.lr_lambdas = list(map(wrappers_builder, self._scheduler.lr_lambdas))\r\n```\r\nI've taken special care to preserve picklability, however, since `LambdaLR` instances created by `transformers` library hold lambda and local functions in them, pickling of `DynamicScheduler` (as well as it's state, which is the same as the wrapped `LambdaLR` state) fails.\r\n\r\nWhile reimplementing dynamic scheduling with lambda functions will allow the `torch` workaround that handles lambda functions in scheduler, the whole point of dynamic scheduling will be lost since the complex dynamically constructed lambdas: `f_n(f_n-1(...f_1(schedule(x))...))` will fall back to their default state: `schedule(x)`.\r\n\r\nHere is the callback I use to track evaluation metrics for anyone interested:\r\n```python\r\ndef get_warmup_steps(args: TrainingArguments, state: TrainerState) -> int:\r\n    return (\r\n        args.warmup_steps\r\n        if args.warmup_steps > 0\r\n        else math.ceil(state.max_steps * args.warmup_ratio)\r\n    )\r\n\r\n\r\nclass DecreaseLRTransformer:\r\n    def __init__(self, decrease_ratio: float):\r\n        if decrease_ratio < 0.0 or decrease_ratio > 1.0:\r\n            raise ValueError('Decrease ratio should be within [1.0, 0.0]')\r\n        self._decrease_ratio = decrease_ratio\r\n\r\n    def __call__(self, lr: float):\r\n        return self._decrease_ratio * lr\r\n\r\n\r\n# Developer notice (may change in the future versions of transformers):\r\n# All kwargs have the following fields set: model, tokenizer, optimizer, lr_scheduler, train_dataloader, eval_dataloader\r\n\r\nclass LRDecreaseCallback(TrainerCallback):\r\n    \"\"\"\r\n    A [`TrainerCallback`] that handles learning rate decrease based on evaluation metrics.\r\n    \"\"\"\r\n\r\n    def __init__(self, decrease_ratio: float, patience: int, *, decrease_on_warmup: bool = False, decrease_threshold: float = 0.0):\r\n        self._transformer = DecreaseLRTransformer(decrease_ratio)\r\n        self._patience = patience\r\n        self._decrease_on_warmup = decrease_on_warmup\r\n        self._decrease_threshold = decrease_threshold\r\n\r\n        self._failed_checks = 0\r\n\r\n    def _metric_improved(self, new_metric: float, old_metric: float, *, greater_is_better: bool = True) -> bool:\r\n        operator = np.greater if greater_is_better else np.less\r\n        return operator(new_metric, old_metric) and abs(new_metric - old_metric) > self._decrease_threshold\r\n\r\n    def check_metric_value(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metric_value: float):\r\n        # best_metric is set by code for load_best_model\r\n        no_metric = (state.best_metric is None)\r\n\r\n        warmup_steps = get_warmup_steps(args, state)\r\n        skip_warmup = (self._decrease_on_warmup and warmup_steps >= state.global_step)\r\n\r\n        if skip_warmup:\r\n            return\r\n\r\n        if no_metric or self._metric_improved(metric_value, state.best_metric, greater_is_better=args.greater_is_better):\r\n            self._failed_checks = 0\r\n            control.should_save = True\r\n        else:\r\n            self._failed_checks += 1\r\n\r\n    def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\r\n        if args.metric_for_best_model is None:\r\n            raise ValueError(f\"{self.__class__.__name__} requires metric_for_best_model to be defined defined\")\r\n\r\n        if args.evaluation_strategy == IntervalStrategy.NO:\r\n            raise ValueError(f\"{self.__class__.__name__} requires IntervalStrategy of steps or epoch\")\r\n\r\n    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\r\n        metrics: Dict[str, float] = kwargs['metrics']\r\n        lr_scheduler = kwargs['lr_scheduler']\r\n        if not isinstance(lr_scheduler, DynamicScheduler):\r\n            logger.warning(f'{self.__class__.__name__} is not compatible with {lr_scheduler.__class__.__name__} scheduler! '\r\n                           f'Wrap your scheduler with {DynamicScheduler.__class__.__name__} to change LR dynamically. '\r\n                           f'{self.__class__.__name__} is disabled!')\r\n            return\r\n\r\n        metric_to_check = args.metric_for_best_model\r\n        if not metric_to_check.startswith(\"eval_\"):\r\n            metric_to_check = f\"eval_{metric_to_check}\"\r\n        metric_value = metrics.get(metric_to_check)\r\n\r\n        if metric_value is None:\r\n            logger.warning(f\"{self.__class__.__name__} required metric_for_best_model, \"\r\n                           f\"but did not find {metric_to_check} in evaluation metrics. {self.__class__.__name__} is disabled!\")\r\n            return\r\n\r\n        self.check_metric_value(args, state, control, metric_value)\r\n        if self._failed_checks >= self._patience:\r\n            lr_scheduler.wrap_schedule(self._transformer)\r\n            self._failed_checks = 0\r\n\r\n    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\r\n        logs: Dict[str, float] = kwargs['logs']\r\n        logs['lr_decrease_patience'] = (self._patience - self._failed_checks) / self._patience\r\n``` \n\n### Your contribution\n\nThe simplest and the cleanest workaround would be to make the local functions global:\r\n\r\nIntead of:\r\n```python\r\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\r\n    def lr_lambda(current_step: int):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1, num_warmup_steps))\r\n        return max(\r\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\r\n        )\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\r\n```\r\n\r\nDo this:\r\n```python\r\ndef _linear_schedule_with_warmup_step(current_step: int, *, num_warmup_steps: int, num_training_steps: int) -> float:\r\n      if current_step < num_warmup_steps:\r\n          return float(current_step) / float(max(1, num_warmup_steps))\r\n      return max(\r\n          0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\r\n      )\r\n\r\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\r\n    schedule = partial(_linear_schedule_with_warmup_step, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\r\n    return LambdaLR(optimizer, schedule, last_epoch)\r\n```\r\nWhen created with global functions, partial function are picklable:\r\n```python\r\n>>>from functools import partial\r\n\r\n>>>import pickle\r\n\r\n>>>def f(x):\r\n...    print(x)\r\n    \r\n>>>with open('f.pkl', 'wb') as file:\r\n...    pickle.dump(partial(f, x='Dog'), file)\r\n    \r\n>>>with open('f.pkl', 'rb') as file:\r\n...    unpickled_f = pickle.load(file)\r\n    \r\n>>>unpickled_f()\r\nDog\r\n```\r\n\r\nThe fix is straightforward and I can create a PR. Nonetheless, it would be my first contribution so I might need some help along the way.\n"
  },
  "gold_slice": {
    "file": [
      "src/transformers/optimization.py"
    ]
  },
  "gold_patch": {
    "diff_uri": "patches/SWE-PolyBench__python__evolution__feature__b4302840.diff"
  },
  "build": {
    "cmd_test": "pytest -v --tb=short --show-capture=no /testbed/tests/optimization/test_optimization.py --junitxml=test-results.xml"
  },
  "verification": {
    "kind": [
      "unit"
    ]
  },
  "original_instance_id": "huggingface__transformers-21768"
}