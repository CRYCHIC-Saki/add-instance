{
  "instance_id": "SWE-PolyBench/python/maintenance/bugfix/2a9201a9",
  "version": "v0.1",
  "category": "maintenance",
  "subcategory": "bugfix",
  "languages": [
    "python"
  ],
  "repo": {
    "origin": "github://huggingface/transformers",
    "commit_pre": "a3aabc702e1c49243e7b48f22d88362d50e786c5"
  },
  "instruction": {
    "type": "issue",
    "text": "[BUG] DataCollatorForSeq2Seq with PaddingStrategy.MAX_LENGTH may not pad labels\n\r\nIt seems that when padding, if the MAX_LENGTH policy is set, the same padding is not performed on the label.\r\n\r\ntest case belowï¼š\r\n```python\r\nfrom transformers import DataCollatorForSeq2Seq,\r\nfrom transformers.utils import PaddingStrategy\r\ninputs=[{'input_ids': [151644, 8948, 198],'attention_mask': [1, 1, 1],'labels': [-100, -100, -100]},\r\n {'input_ids': [151644, 8948, 198, 2610],'attention_mask': [1, 1, 1, 1],'labels': [-100, -100, -100, -100]},\r\n {'input_ids': [151644, 8948, 198, 2610, 525], 'attention_mask': [1, 1, 1, 1, 1],'labels': [-100, -100, -100, -100, -100]}]\r\ndata_collator = DataCollatorForSeq2Seq(\r\n        tokenizer=tokenizer,\r\n        padding=PaddingStrategy.MAX_LENGTH,\r\n        max_length=10,\r\n    )\r\nres=data_collator(inputs)\r\n\r\nprint(res['input_ids'].shape,res['labels'].shape)\r\n```\r\n\r\nresults:\r\ntorch.Size([3, 10]) torch.Size([3, 5])\r\n\r\nexpected results:\r\ntorch.Size([3, 10]) torch.Size([3, 10])\r\n\r\nShould the following code handle the pad length of the label according to different strategies?\r\nhttps://github.com/huggingface/transformers/blob/73014b561d5f88d728e46a57d346f516fefe3f2d/src/transformers/data/data_collator.py#L592\r\n\n"
  },
  "gold_slice": {
    "file": [
      "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
      "src/transformers/data/data_collator.py"
    ]
  },
  "gold_patch": {
    "diff_uri": "patches/SWE-PolyBench__python__maintenance__bugfix__2a9201a9.diff"
  },
  "build": {
    "cmd_test": "pytest -v --tb=short --show-capture=no --json-report /testbed/tests/trainer/test_data_collator.py"
  },
  "verification": {
    "kind": [
      "unit"
    ]
  },
  "original_instance_id": "huggingface__transformers-30556"
}